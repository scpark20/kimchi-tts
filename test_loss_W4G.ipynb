{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from hparams.hparams_W4G import create_hparams\n",
    "from model import Model\n",
    "from datasets import LJDataset, TextMelCollate\n",
    "from utils import sizeof_fmt, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 19 12:55:43 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:2E:00.0 Off |                  N/A |\r\n",
      "| 37%   61C    P0    73W / 250W |      0MiB / 11016MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:2F:00.0 Off |                  N/A |\r\n",
      "| 53%   62C    P2   187W / 250W |   3645MiB / 11019MiB |     82%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1   N/A  N/A     15524      C   ...onda3/envs/vae/bin/python     1843MiB |\r\n",
      "|    1   N/A  N/A     15526      C   ...onda3/envs/vae/bin/python     1799MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/data/save/model_W4G'\n",
    "logger = Logger(save_dir=save_dir, new=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.json    save_101436  save_102141  save_400000\r\n",
      "save_100000  save_101938  save_102658\r\n"
     ]
    }
   ],
   "source": [
    "!ls $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size 124.1MiB\n",
      "TTS size 36.5MiB\n",
      "MelDecoder size 22.9MiB\n",
      "loaded : 400000\n",
      "400000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stt_hparams, tts_hparams = create_hparams()\n",
    "model = Model(stt_hparams, tts_hparams, mode='train')\n",
    "model = model.cuda()\n",
    "step = 400000\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.parameters()))\n",
    "print(f\"Model size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.parameters()))\n",
    "print(f\"TTS size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.mel_decoder.parameters()))\n",
    "print(f\"MelDecoder size {size}\")\n",
    "\n",
    "if True:\n",
    "    model, _, _ = logger.load(step, model, None)\n",
    "print(step)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7ff0e4387fd0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7ff0609bbe80>\n"
     ]
    }
   ],
   "source": [
    "trainset = LJDataset(tts_hparams, split='train')\n",
    "testset = LJDataset(tts_hparams, split='valid')\n",
    "collate_fn = TextMelCollate(tts_hparams)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, num_workers=1, #tts_hparams.num_workers, \n",
    "                          shuffle=True, sampler=None, batch_size=tts_hparams.batch_size, pin_memory=False,\n",
    "                          drop_last=True, collate_fn=collate_fn)\n",
    "print(train_loader)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, num_workers=1, \n",
    "                          shuffle=False, sampler=None, batch_size=1, pin_memory=False,\n",
    "                          drop_last=True, collate_fn=collate_fn)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "    batch['text'] = batch['text'].cuda()\n",
    "    batch['text_lengths'] = batch['text_lengths'].cuda()\n",
    "    batch['mels'] = batch['mels'].cuda()\n",
    "    batch['mel_lengths'] = batch['mel_lengths'].cuda()\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.02667085826396942\n",
      "1 0.023400668054819107\n",
      "2 0.023837624117732048\n",
      "3 0.02691640332341194\n",
      "4 0.02382315695285797\n",
      "5 0.025670312345027924\n",
      "6 0.02382320538163185\n",
      "7 0.02772974595427513\n",
      "8 0.02745291404426098\n",
      "9 0.025503579527139664\n",
      "10 0.02708583138883114\n",
      "11 0.025199536234140396\n",
      "12 0.026328686624765396\n",
      "13 0.02591957524418831\n",
      "14 0.027820836752653122\n",
      "15 0.0263991542160511\n",
      "16 0.023914894089102745\n",
      "17 0.024729019030928612\n",
      "18 0.02718309499323368\n",
      "19 0.026684589684009552\n",
      "20 0.02692275308072567\n",
      "21 0.027878224849700928\n",
      "22 0.025882594287395477\n",
      "23 0.027538947761058807\n",
      "24 0.025294728577136993\n",
      "25 0.027478905394673347\n",
      "26 0.027889825403690338\n",
      "27 0.027582192793488503\n",
      "28 0.023624958470463753\n",
      "29 0.029210040345788002\n",
      "30 0.028289226815104485\n",
      "31 0.025709498673677444\n",
      "32 0.0254405215382576\n",
      "33 0.025958798825740814\n",
      "34 0.027683133259415627\n",
      "35 0.02773411199450493\n",
      "36 0.026453088968992233\n",
      "37 0.025053178891539574\n",
      "38 0.024989325553178787\n",
      "39 0.02535027079284191\n",
      "40 0.027588099241256714\n",
      "41 0.026543449610471725\n",
      "42 0.025030627846717834\n",
      "43 0.028197091072797775\n",
      "44 0.02881813421845436\n",
      "45 0.02624652162194252\n",
      "46 0.0251935888081789\n",
      "47 0.02483334392309189\n",
      "48 0.02720114402472973\n",
      "49 0.02676636166870594\n",
      "50 0.025105386972427368\n",
      "51 0.026764657348394394\n",
      "52 0.025734925642609596\n",
      "53 0.026058059185743332\n",
      "54 0.02454238198697567\n",
      "55 0.025434963405132294\n",
      "56 0.022950664162635803\n",
      "57 0.02879924140870571\n",
      "58 0.0272306390106678\n",
      "59 0.022973092272877693\n",
      "60 0.026823824271559715\n",
      "61 0.02321675233542919\n",
      "62 0.026468554511666298\n",
      "63 0.028822530061006546\n",
      "64 0.028138138353824615\n",
      "65 0.023965511471033096\n",
      "66 0.026088451966643333\n",
      "67 0.024586329236626625\n",
      "68 0.024323074147105217\n",
      "69 0.029258981347084045\n",
      "70 0.02634323015809059\n",
      "71 0.025641564279794693\n",
      "72 0.024294856935739517\n",
      "73 0.025695301592350006\n",
      "74 0.027646351605653763\n",
      "75 0.024882342666387558\n",
      "76 0.027233092114329338\n",
      "77 0.028293512761592865\n",
      "78 0.0275654848664999\n",
      "79 0.024714378640055656\n",
      "80 0.026339417323470116\n",
      "81 0.028279904276132584\n",
      "82 0.027257949113845825\n",
      "83 0.027708230540156364\n",
      "84 0.02762174978852272\n",
      "85 0.026682941243052483\n",
      "86 0.027020713314414024\n",
      "87 0.025872191414237022\n",
      "88 0.027014190331101418\n",
      "89 0.028468528762459755\n",
      "90 0.026308970525860786\n",
      "91 0.025377769023180008\n",
      "92 0.024673791602253914\n",
      "93 0.025235794484615326\n",
      "94 0.02172745019197464\n",
      "95 0.025669777765870094\n",
      "96 0.026764018461108208\n",
      "97 0.027108509093523026\n",
      "98 0.02509385719895363\n",
      "99 0.026619624346494675\n",
      "100 0.02440739795565605\n",
      "101 0.02546321041882038\n",
      "102 0.027790065854787827\n",
      "103 0.025767900049686432\n",
      "104 0.02609725296497345\n",
      "105 0.026179315522313118\n",
      "106 0.027095602825284004\n",
      "107 0.02807151898741722\n",
      "108 0.028052184730768204\n",
      "109 0.027174193412065506\n",
      "110 0.024919966235756874\n",
      "111 0.026861783117055893\n",
      "112 0.02479187585413456\n",
      "113 0.0275497455149889\n",
      "114 0.026095477864146233\n",
      "115 0.026681741699576378\n",
      "116 0.030543245375156403\n",
      "117 0.027877332642674446\n",
      "118 0.02752259559929371\n",
      "119 0.024074800312519073\n",
      "120 0.026852957904338837\n",
      "121 0.026445167139172554\n",
      "122 0.026644283905625343\n",
      "123 0.027998987585306168\n",
      "124 0.026260962709784508\n",
      "125 0.026986710727214813\n",
      "126 0.02596849761903286\n",
      "127 0.02562386728823185\n",
      "128 0.025326307862997055\n",
      "129 0.02372845448553562\n",
      "130 0.026580141857266426\n",
      "131 0.025235069915652275\n",
      "132 0.026062151417136192\n",
      "133 0.02837122604250908\n",
      "134 0.023536210879683495\n",
      "135 0.024087706580758095\n",
      "136 0.02630586363375187\n",
      "137 0.02575133740901947\n",
      "138 0.02526101842522621\n",
      "139 0.025886012241244316\n",
      "140 0.026811672374606133\n",
      "141 0.025336526334285736\n",
      "142 0.027646629139780998\n",
      "143 0.027983887121081352\n",
      "144 0.027707163244485855\n",
      "145 0.025476856157183647\n",
      "146 0.025141891092061996\n",
      "147 0.02607969380915165\n",
      "148 0.024168694391846657\n",
      "149 0.024716626852750778\n",
      "150 0.027577951550483704\n",
      "151 0.025640785694122314\n",
      "152 0.026715682819485664\n",
      "153 0.028870591893792152\n",
      "154 0.02749268338084221\n",
      "155 0.024263974279165268\n",
      "156 0.02768930420279503\n",
      "157 0.027513941749930382\n",
      "158 0.02661234512925148\n",
      "159 0.025230642408132553\n",
      "160 0.026330003514885902\n",
      "161 0.025719331577420235\n",
      "162 0.026235247030854225\n",
      "163 0.027195004746317863\n",
      "164 0.025821736082434654\n",
      "165 0.02634153515100479\n",
      "166 0.024969086050987244\n",
      "167 0.02647174336016178\n",
      "168 0.026358535513281822\n",
      "169 0.023958249017596245\n",
      "170 0.022875793278217316\n",
      "171 0.027950579300522804\n",
      "172 0.024962637573480606\n",
      "173 0.024922940880060196\n",
      "174 0.025812791660428047\n",
      "175 0.027053242549300194\n",
      "176 0.02492259256541729\n",
      "177 0.026176894083619118\n",
      "178 0.025660311803221703\n",
      "179 0.027485089376568794\n",
      "180 0.024176103994250298\n",
      "181 0.02404106594622135\n",
      "182 0.02536728046834469\n",
      "183 0.024222664535045624\n",
      "184 0.025691211223602295\n",
      "185 0.025612536817789078\n",
      "186 0.02669856697320938\n",
      "187 0.027239441871643066\n",
      "188 0.028185764327645302\n",
      "189 0.026749851182103157\n",
      "190 0.024785391986370087\n",
      "191 0.025893554091453552\n",
      "192 0.02691853791475296\n",
      "193 0.026702092960476875\n",
      "194 0.026963604614138603\n",
      "195 0.023919055238366127\n",
      "196 0.025845816358923912\n",
      "197 0.025339480489492416\n",
      "198 0.02626630663871765\n",
      "199 0.02439871057868004\n",
      "200 0.025881730020046234\n",
      "201 0.025943361222743988\n",
      "202 0.028667766600847244\n",
      "203 0.027937138453125954\n",
      "204 0.02652554400265217\n",
      "205 0.02587212435901165\n",
      "206 0.027865545824170113\n",
      "207 0.025194339454174042\n",
      "208 0.02652791142463684\n",
      "209 0.027764545753598213\n",
      "210 0.026824427768588066\n",
      "211 0.025142187252640724\n",
      "212 0.027300266548991203\n",
      "213 0.027711812406778336\n",
      "214 0.025477642193436623\n",
      "215 0.02507387287914753\n",
      "216 0.027441024780273438\n",
      "217 0.02745000086724758\n",
      "218 0.026136327534914017\n",
      "219 0.025127220898866653\n",
      "220 0.023484786972403526\n",
      "221 0.02643914893269539\n",
      "222 0.026759199798107147\n",
      "223 0.027649281546473503\n",
      "224 0.026751013472676277\n",
      "225 0.02509952150285244\n",
      "226 0.02673913910984993\n",
      "227 0.02535078302025795\n",
      "228 0.026341557502746582\n",
      "229 0.025092381983995438\n",
      "230 0.02607184648513794\n",
      "231 0.02747705765068531\n",
      "232 0.02788727544248104\n",
      "233 0.026569388806819916\n",
      "234 0.024943290278315544\n",
      "235 0.02819734252989292\n",
      "236 0.025877291336655617\n",
      "237 0.02647073194384575\n",
      "238 0.027081768959760666\n",
      "239 0.024602718651294708\n",
      "240 0.025429734960198402\n",
      "241 0.027053389698266983\n",
      "242 0.02803330309689045\n",
      "243 0.026680950075387955\n",
      "244 0.024774130433797836\n",
      "245 0.025053519755601883\n",
      "246 0.02531585656106472\n",
      "247 0.02502935193479061\n",
      "248 0.026227381080389023\n",
      "249 0.024125389754772186\n",
      "250 0.02694462612271309\n",
      "251 0.02541978843510151\n",
      "252 0.026727871969342232\n",
      "253 0.02581171505153179\n",
      "254 0.0269576758146286\n",
      "255 0.027219504117965698\n",
      "256 0.02765761874616146\n",
      "257 0.02527817338705063\n",
      "258 0.025381790474057198\n",
      "259 0.025112273171544075\n",
      "260 0.02693629451096058\n",
      "261 0.026050861924886703\n",
      "262 0.024714607745409012\n",
      "263 0.026712989434599876\n",
      "264 0.025027116760611534\n",
      "265 0.026224810630083084\n",
      "266 0.02615456096827984\n",
      "267 0.025313498452305794\n",
      "268 0.02553115040063858\n",
      "269 0.02732078544795513\n",
      "270 0.023563195019960403\n",
      "271 0.026445677503943443\n",
      "272 0.027547277510166168\n",
      "273 0.027228469029068947\n",
      "274 0.023079978302121162\n",
      "275 0.028343483805656433\n",
      "276 0.029172495007514954\n",
      "277 0.02465309388935566\n",
      "278 0.026253877207636833\n",
      "279 0.027388688176870346\n",
      "280 0.025533100590109825\n",
      "281 0.026023276150226593\n",
      "282 0.025593867525458336\n",
      "283 0.025074027478694916\n",
      "284 0.027339916676282883\n",
      "285 0.02333841286599636\n",
      "286 0.02420062944293022\n",
      "287 0.025637051090598106\n",
      "288 0.02654767967760563\n",
      "289 0.025525659322738647\n",
      "290 0.02649138681590557\n",
      "291 0.025869933888316154\n",
      "292 0.027334969490766525\n",
      "293 0.026780320331454277\n",
      "294 0.02376994676887989\n",
      "295 0.0248765517026186\n",
      "296 0.027308575809001923\n",
      "297 0.028443951159715652\n",
      "298 0.024598071351647377\n",
      "299 0.026706434786319733\n",
      "300 0.02472194842994213\n",
      "301 0.02573711983859539\n",
      "302 0.02746805176138878\n",
      "303 0.025377942249178886\n",
      "304 0.02465648762881756\n",
      "305 0.03111911006271839\n",
      "306 0.028964048251509666\n",
      "307 0.027115438133478165\n",
      "308 0.02603819966316223\n",
      "309 0.025036020204424858\n",
      "310 0.02622530236840248\n",
      "311 0.025395939126610756\n",
      "312 0.02696310356259346\n",
      "313 0.02495536394417286\n",
      "314 0.025678403675556183\n",
      "315 0.024823149666190147\n",
      "316 0.027412762865424156\n",
      "317 0.026456119492650032\n",
      "318 0.02791144698858261\n",
      "319 0.025468720123171806\n",
      "320 0.026698289439082146\n",
      "321 0.025555597618222237\n",
      "322 0.028193138539791107\n",
      "323 0.026914432644844055\n",
      "324 0.0245803352445364\n",
      "325 0.02811465412378311\n",
      "326 0.028682857751846313\n",
      "327 0.027100395411252975\n",
      "328 0.02628844790160656\n",
      "329 0.027692893519997597\n",
      "330 0.02438359521329403\n",
      "331 0.024067537859082222\n",
      "332 0.026782860979437828\n",
      "333 0.027996791526675224\n",
      "334 0.026393450796604156\n",
      "335 0.026103265583515167\n",
      "336 0.027774309739470482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 0.02441144548356533\n",
      "338 0.025161778554320335\n",
      "339 0.0252109132707119\n",
      "340 0.02484969049692154\n",
      "341 0.025519205257296562\n",
      "342 0.026870617642998695\n",
      "343 0.027341043576598167\n",
      "344 0.024248510599136353\n",
      "345 0.025592323392629623\n",
      "346 0.027269523590803146\n",
      "347 0.02558806538581848\n",
      "348 0.027248511090874672\n",
      "349 0.029391568154096603\n",
      "350 0.024167723953723907\n",
      "351 0.023876497521996498\n",
      "352 0.02791108749806881\n",
      "353 0.024208970367908478\n",
      "354 0.02634073793888092\n",
      "355 0.027660205960273743\n",
      "356 0.02697041817009449\n",
      "357 0.02435673587024212\n",
      "358 0.02516515925526619\n",
      "359 0.024938052520155907\n",
      "360 0.025893010199069977\n",
      "361 0.023968210443854332\n",
      "362 0.02672012336552143\n",
      "363 0.02571903169155121\n",
      "364 0.0235916581004858\n",
      "365 0.024370871484279633\n",
      "366 0.026865074411034584\n",
      "367 0.02603326365351677\n",
      "368 0.027656931430101395\n",
      "369 0.026097552850842476\n",
      "370 0.02479543536901474\n",
      "371 0.024610823020339012\n",
      "372 0.027288910001516342\n",
      "373 0.024811336770653725\n",
      "374 0.026877425611019135\n",
      "375 0.02671832963824272\n",
      "376 0.02656613290309906\n",
      "377 0.026910964399576187\n",
      "378 0.025014765560626984\n",
      "379 0.025349678471684456\n",
      "380 0.026941506192088127\n",
      "381 0.023654671385884285\n",
      "done\n",
      "0.00016205459371159927\n",
      "0.026159709836331962\n",
      "0.008762963097883847\n",
      "1.0445970916311154\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "ce_losses = []\n",
    "recon_losses = []\n",
    "kl_losses = []\n",
    "sample_stds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = to_cuda(batch)\n",
    "        stt_outputs, tts_outputs = model(batch)\n",
    "        ce_loss = stt_outputs['loss'].item()\n",
    "        recon_loss = tts_outputs['recon_loss'].item()\n",
    "        kl_loss = tts_outputs['kl_loss'].item()\n",
    "        print(i, recon_loss)\n",
    "        \n",
    "        ce_losses.append(ce_loss)\n",
    "        recon_losses.append(recon_loss)\n",
    "        kl_losses.append(kl_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        samples_list = []\n",
    "        for _ in range(10):\n",
    "            samples, _ = model.inference(batch['text'], batch['mels'].size(2), stt_outputs[\"alignments\"], temperature=1.0, clip=2)\n",
    "            samples_list.append(samples)\n",
    "        samples_list = torch.cat(samples_list, dim=0)\n",
    "        sample_std = torch.std(samples_list, dim=0).mean().item()\n",
    "        sample_stds.append(sample_std)\n",
    "            \n",
    "        \n",
    "print('done')\n",
    "\n",
    "print(np.mean(ce_losses))\n",
    "print(np.mean(recon_losses))\n",
    "print(np.mean(kl_losses))\n",
    "print(np.mean(sample_stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045737755880958735\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(recon_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
