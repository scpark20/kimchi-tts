{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from hparams.normal2_hparams import create_hparams\n",
    "from model import Model\n",
    "from datasets import LJDataset, TextMelCollate\n",
    "from utils import sizeof_fmt, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar  6 19:38:25 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:2E:00.0 Off |                  N/A |\r\n",
      "| 61%   71C    P0   103W / 250W |      0MiB / 11016MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:2F:00.0 Off |                  N/A |\r\n",
      "| 37%   63C    P0    50W / 250W |      0MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'save/normal2_model'\n",
    "logger = Logger(save_dir=save_dir, new=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size 64.9MiB\n",
      "TTS size 58.6MiB\n",
      "MelEncoder size 18.3MiB\n",
      "MelDecoder size 38.4MiB\n",
      "0\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stt_hparams, tts_hparams = create_hparams()\n",
    "model = Model(stt_hparams, tts_hparams)\n",
    "model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=tts_hparams.lr, weight_decay=tts_hparams.weight_decay)\n",
    "\n",
    "step = 0\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.parameters()))\n",
    "print(f\"Model size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.parameters()))\n",
    "print(f\"TTS size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.mel_encoder.parameters()))\n",
    "print(f\"MelEncoder size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.mel_decoder.parameters()))\n",
    "print(f\"MelDecoder size {size}\")\n",
    "\n",
    "if False:\n",
    "    model, optimizer, step = logger.load(step, model, optimizer)\n",
    "print(step)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f8ea4d5ca30>\n"
     ]
    }
   ],
   "source": [
    "trainset = LJDataset(tts_hparams.root_dir)\n",
    "collate_fn = TextMelCollate()\n",
    "train_loader = torch.utils.data.DataLoader(trainset, num_workers=tts_hparams.num_workers, \n",
    "                          shuffle=True,\n",
    "                          sampler=None,\n",
    "                          batch_size=tts_hparams.batch_size, pin_memory=False,\n",
    "                          drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "    batch['text'] = batch['text'].cuda()\n",
    "    batch['text_lengths'] = batch['text_lengths'].cuda()\n",
    "    batch['mels'] = batch['mels'].cuda()\n",
    "    batch['mel_lengths'] = batch['mel_lengths'].cuda()\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = to_cuda(batch)\n",
    "        \n",
    "        model.train()\n",
    "        model.increase_step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        stt_outputs, tts_outputs = model(batch)\n",
    "        loss = tts_outputs['loss'] + stt_outputs['loss']\n",
    "        loss.backward()\n",
    "        \n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        print(grad_norm.mean())\n",
    "        if torch.isnan(grad_norm.mean()):\n",
    "            continue\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 5000 == 0:\n",
    "            logger.save(step, model, optimizer)\n",
    "        \n",
    "        if step % 1 == 0:\n",
    "            print('step :', step, \n",
    "                  'stt :', '%0.4f' % stt_outputs['loss'].item(),\n",
    "                  'tts :', '%0.4f' % tts_outputs['loss'].item(),\n",
    "                  'recon :', '%0.4f' % tts_outputs['recon_loss'].item(),\n",
    "                  'kl :', '%0.4f' % tts_outputs['kl_loss'].item(),\n",
    "                  'beta :', '%0.4f' % model.beta.item())\n",
    "                  \n",
    "            \n",
    "        if step % 100 == 0:\n",
    "            logger.log(step, 'stt_loss', stt_outputs['loss'].item())\n",
    "            logger.log(step, 'tts_loss', tts_outputs['loss'].item())    \n",
    "            logger.log(step, 'recon_loss', tts_outputs['recon_loss'].item())    \n",
    "            logger.log(step, 'kl_loss', tts_outputs['kl_loss'].item())    \n",
    "            logger.log(step, 'beta', model.beta.item())    \n",
    "            \n",
    "        if step % 100 == 0:\n",
    "            display.clear_output()\n",
    "            \n",
    "            index = 3\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                samples = model.inference(batch['text'], batch['mels'].size(2), stt_outputs[\"alignments\"], temperature=0.7)\n",
    "\n",
    "            _tts_alignments = tts_outputs[\"alignments\"].data.cpu().numpy()\n",
    "            batch_size = _tts_alignments.shape[0]\n",
    "            plt.figure(figsize=[18, 3])\n",
    "            for i in range(3):\n",
    "                plt.subplot(1, 3, i+1)\n",
    "                plt.imshow(_tts_alignments[i].T, aspect='auto', origin='lower', interpolation='none')\n",
    "            plt.show()\n",
    "            \n",
    "            _stt_alignments = stt_outputs[\"alignments\"].data.cpu().numpy()\n",
    "            batch_size = _stt_alignments.shape[0]\n",
    "            plt.figure(figsize=[18, 3])\n",
    "            for i in range(3):\n",
    "                plt.subplot(1, 3, i+1)\n",
    "                plt.imshow(_stt_alignments[i].T, aspect='auto', origin='lower', interpolation='none')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=[18, 3])\n",
    "            _x = batch['mels'].data.cpu().numpy()\n",
    "            librosa.display.specshow(_x[index])\n",
    "            plt.colorbar()\n",
    "            \n",
    "            for i in range(_stt_alignments.shape[1]):\n",
    "                plt.plot(_stt_alignments[index, i] * 100)\n",
    "                \n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=[18, 3])\n",
    "            _y = tts_outputs['pred'].data.cpu().numpy()\n",
    "            librosa.display.specshow(_y[index])\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            \n",
    "            plt.figure(figsize=[18, 3])\n",
    "            _s = samples.data.cpu().numpy()\n",
    "            librosa.display.specshow(_s[index, :, :_y.shape[2]])\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "                      \n",
    "        step += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.save(step, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    index = 0\n",
    "    samples = model.inference(batch['text'], batch['mels'].size(2), stt_outputs[\"alignments\"], temperature=1.0)\n",
    "    print(samples.shape)\n",
    "    \n",
    "    plt.figure(figsize=[18, 3])\n",
    "    _s = samples.data.cpu().numpy()\n",
    "    librosa.display.specshow(_s[index])\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
