{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from hparams.hparams_S5G import create_hparams\n",
    "from model import Model\n",
    "from datasets import LJDataset, TextMelCollate\n",
    "from utils import sizeof_fmt, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 19 12:55:20 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:2E:00.0 Off |                  N/A |\r\n",
      "| 43%   61C    P0    74W / 250W |      0MiB / 11016MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:2F:00.0 Off |                  N/A |\r\n",
      "| 48%   56C    P2   133W / 250W |   1802MiB / 11019MiB |     41%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    1   N/A  N/A     15526      C   ...onda3/envs/vae/bin/python     1799MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/data/save/model_S5G'\n",
    "logger = Logger(save_dir=save_dir, new=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.json    save_135000  save_185000  save_239735  save_290000  save_360000\r\n",
      "save_100000  save_137981  save_185146  save_239809  save_295000  save_362255\r\n",
      "save_100189  save_138253  save_190000  save_240000  save_300000  save_363423\r\n",
      "save_100481  save_140000  save_195000  save_242209  save_305000  save_365000\r\n",
      "save_100728  save_145000  save_196835  save_245000  save_306501  save_370000\r\n",
      "save_104221  save_146358  save_199144  save_247546  save_310000  save_375000\r\n",
      "save_105000  save_147866  save_200000  save_248082  save_315000  save_380000\r\n",
      "save_110000  save_150000  save_201253  save_250000  save_320000  save_385000\r\n",
      "save_115000  save_151422  save_205000  save_255000  save_324281  save_387320\r\n",
      "save_120000  save_155000  save_210000  save_258351  save_325000  save_400000\r\n",
      "save_123593  save_160000  save_215000  save_258531  save_330000  save_75000\r\n",
      "save_123671  save_165000  save_219392  save_260000  save_335000  save_80000\r\n",
      "save_123692  save_170000  save_220000  save_265000  save_340000  save_85000\r\n",
      "save_125000  save_170017  save_225000  save_270000  save_344332  save_90000\r\n",
      "save_128431  save_175000  save_228147  save_275000  save_345000  save_95000\r\n",
      "save_130000  save_177763  save_230000  save_280000  save_350000  save_98268\r\n",
      "save_132767  save_180000  save_232353  save_282023  save_353107  save_98508\r\n",
      "save_133754  save_184582  save_235000  save_285000  save_355000\r\n"
     ]
    }
   ],
   "source": [
    "!ls $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size 283.0MiB\n",
      "TTS size 195.3MiB\n",
      "MelDecoder size 119.0MiB\n",
      "loaded : 400000\n",
      "400000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stt_hparams, tts_hparams = create_hparams()\n",
    "model = Model(stt_hparams, tts_hparams, mode='train')\n",
    "model = model.cuda()\n",
    "step = 400000\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.parameters()))\n",
    "print(f\"Model size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.parameters()))\n",
    "print(f\"TTS size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.mel_decoder.parameters()))\n",
    "print(f\"MelDecoder size {size}\")\n",
    "\n",
    "if True:\n",
    "    model, _, _ = logger.load(step, model, None)\n",
    "print(step)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f19548e8fa0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f194951cd60>\n"
     ]
    }
   ],
   "source": [
    "trainset = LJDataset(tts_hparams, split='train')\n",
    "testset = LJDataset(tts_hparams, split='valid')\n",
    "collate_fn = TextMelCollate(tts_hparams)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, num_workers=1, #tts_hparams.num_workers, \n",
    "                          shuffle=True, sampler=None, batch_size=tts_hparams.batch_size, pin_memory=False,\n",
    "                          drop_last=True, collate_fn=collate_fn)\n",
    "print(train_loader)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, num_workers=1, \n",
    "                          shuffle=False, sampler=None, batch_size=1, pin_memory=False,\n",
    "                          drop_last=True, collate_fn=collate_fn)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "    batch['text'] = batch['text'].cuda()\n",
    "    batch['text_lengths'] = batch['text_lengths'].cuda()\n",
    "    batch['mels'] = batch['mels'].cuda()\n",
    "    batch['mel_lengths'] = batch['mel_lengths'].cuda()\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.022026753053069115\n",
      "1 0.023143725469708443\n",
      "2 0.022638076916337013\n",
      "3 0.019688107073307037\n",
      "4 0.022250201553106308\n",
      "5 0.021832916885614395\n",
      "6 0.021876441314816475\n",
      "7 0.023222211748361588\n",
      "8 0.022262725979089737\n",
      "9 0.023279467597603798\n",
      "10 0.025753909721970558\n",
      "11 0.02520756423473358\n",
      "12 0.021154791116714478\n",
      "13 0.021061643958091736\n",
      "14 0.022246038541197777\n",
      "15 0.021627606824040413\n",
      "16 0.023495744913816452\n",
      "17 0.02398715540766716\n",
      "18 0.02214910462498665\n",
      "19 0.0219180416315794\n",
      "20 0.021468905732035637\n",
      "21 0.021053005009889603\n",
      "22 0.019942184910178185\n",
      "23 0.02233804762363434\n",
      "24 0.02131296694278717\n",
      "25 0.0219963900744915\n",
      "26 0.023628881201148033\n",
      "27 0.021848473697900772\n",
      "28 0.023121705278754234\n",
      "29 0.021480225026607513\n",
      "30 0.0216981153935194\n",
      "31 0.021800700575113297\n",
      "32 0.023176835849881172\n",
      "33 0.020909324288368225\n",
      "34 0.022546594962477684\n",
      "35 0.022049767896533012\n",
      "36 0.02314898744225502\n",
      "37 0.023999875411391258\n",
      "38 0.024075450375676155\n",
      "39 0.022620564326643944\n",
      "40 0.02205466665327549\n",
      "41 0.023304320871829987\n",
      "42 0.023399602621793747\n",
      "43 0.021548978984355927\n",
      "44 0.023105740547180176\n",
      "45 0.02265472337603569\n",
      "46 0.023324251174926758\n",
      "47 0.0237286239862442\n",
      "48 0.022452522069215775\n",
      "49 0.023056207224726677\n",
      "50 0.02387346513569355\n",
      "51 0.021112855523824692\n",
      "52 0.023529453203082085\n",
      "53 0.02063886821269989\n",
      "54 0.0230044387280941\n",
      "55 0.02127411961555481\n",
      "56 0.023815857246518135\n",
      "57 0.023549985140562057\n",
      "58 0.020996971055865288\n",
      "59 0.021150313317775726\n",
      "60 0.024756675586104393\n",
      "61 0.022096291184425354\n",
      "62 0.022154487669467926\n",
      "63 0.022689450532197952\n",
      "64 0.021715395152568817\n",
      "65 0.021785518154501915\n",
      "66 0.02301386184990406\n",
      "67 0.024008432403206825\n",
      "68 0.021575085818767548\n",
      "69 0.02240767516195774\n",
      "70 0.023140924051404\n",
      "71 0.022283606231212616\n",
      "72 0.022233247756958008\n",
      "73 0.022741852328181267\n",
      "74 0.0237395241856575\n",
      "75 0.02112790010869503\n",
      "76 0.023199599236249924\n",
      "77 0.023032357916235924\n",
      "78 0.02139020338654518\n",
      "79 0.023059697821736336\n",
      "80 0.02164142020046711\n",
      "81 0.020418070256710052\n",
      "82 0.020954037085175514\n",
      "83 0.02173907682299614\n",
      "84 0.023867592215538025\n",
      "85 0.02013874053955078\n",
      "86 0.022871941328048706\n",
      "87 0.022588860243558884\n",
      "88 0.022258298471570015\n",
      "89 0.024373574182391167\n",
      "90 0.02241838164627552\n",
      "91 0.02435876615345478\n",
      "92 0.023640234023332596\n",
      "93 0.02329334430396557\n",
      "94 0.020765043795108795\n",
      "95 0.02288524992763996\n",
      "96 0.02145690657198429\n",
      "97 0.021568475291132927\n",
      "98 0.022428907454013824\n",
      "99 0.02175053395330906\n",
      "100 0.02241341769695282\n",
      "101 0.022149696946144104\n",
      "102 0.02299410291016102\n",
      "103 0.023127803578972816\n",
      "104 0.02581220306456089\n",
      "105 0.022808557376265526\n",
      "106 0.022938625887036324\n",
      "107 0.021420955657958984\n",
      "108 0.02320287935435772\n",
      "109 0.0246664360165596\n",
      "110 0.02119944617152214\n",
      "111 0.023799829185009003\n",
      "112 0.023742446675896645\n",
      "113 0.021666156128048897\n",
      "114 0.023036343976855278\n",
      "115 0.024891294538974762\n",
      "116 0.02197117544710636\n",
      "117 0.022889064624905586\n",
      "118 0.02359149418771267\n",
      "119 0.02270204946398735\n",
      "120 0.023893889039754868\n",
      "121 0.02381082810461521\n",
      "122 0.022472767159342766\n",
      "123 0.02126149833202362\n",
      "124 0.021430011838674545\n",
      "125 0.02439563162624836\n",
      "126 0.02224135771393776\n",
      "127 0.022313492372632027\n",
      "128 0.024802878499031067\n",
      "129 0.021160706877708435\n",
      "130 0.020885342732071877\n",
      "131 0.02008316107094288\n",
      "132 0.021103831008076668\n",
      "133 0.022054852917790413\n",
      "134 0.021457094699144363\n",
      "135 0.020364753901958466\n",
      "136 0.019528703764081\n",
      "137 0.02295578457415104\n",
      "138 0.02400277927517891\n",
      "139 0.02395883947610855\n",
      "140 0.02415349707007408\n",
      "141 0.021999197080731392\n",
      "142 0.023523537442088127\n",
      "143 0.024622511118650436\n",
      "144 0.021587274968624115\n",
      "145 0.023660598322749138\n",
      "146 0.021163634955883026\n",
      "147 0.02337685599923134\n",
      "148 0.022136252373456955\n",
      "149 0.02179803140461445\n",
      "150 0.019564861431717873\n",
      "151 0.023276440799236298\n",
      "152 0.023313449695706367\n",
      "153 0.02297152765095234\n",
      "154 0.025319477543234825\n",
      "155 0.023380203172564507\n",
      "156 0.02326476201415062\n",
      "157 0.021999230608344078\n",
      "158 0.023702962324023247\n",
      "159 0.021632064133882523\n",
      "160 0.020301198586821556\n",
      "161 0.024277884513139725\n",
      "162 0.02359909564256668\n",
      "163 0.022744296118617058\n",
      "164 0.02210378274321556\n",
      "165 0.023422224447131157\n",
      "166 0.021349484100937843\n",
      "167 0.021192267537117004\n",
      "168 0.022399581968784332\n",
      "169 0.02444327622652054\n",
      "170 0.02274595946073532\n",
      "171 0.021088913083076477\n",
      "172 0.020849816501140594\n",
      "173 0.02392319217324257\n",
      "174 0.025035832077264786\n",
      "175 0.023183239623904228\n",
      "176 0.020273923873901367\n",
      "177 0.024002939462661743\n",
      "178 0.02093837782740593\n",
      "179 0.020240547135472298\n",
      "180 0.02217681333422661\n",
      "181 0.02218441106379032\n",
      "182 0.024564668536186218\n",
      "183 0.023698531091213226\n",
      "184 0.023282751441001892\n",
      "185 0.0229308120906353\n",
      "186 0.02326747216284275\n",
      "187 0.021738851442933083\n",
      "188 0.02376893348991871\n",
      "189 0.0228115227073431\n",
      "190 0.021794965490698814\n",
      "191 0.021921779960393906\n",
      "192 0.020915737375617027\n",
      "193 0.024085132405161858\n",
      "194 0.023096108809113503\n",
      "195 0.020231394097208977\n",
      "196 0.021602487191557884\n",
      "197 0.02126186154782772\n",
      "198 0.023260843008756638\n",
      "199 0.02297830395400524\n",
      "200 0.020974552258849144\n",
      "201 0.019370967522263527\n",
      "202 0.02181086875498295\n",
      "203 0.020201243460178375\n",
      "204 0.022718098014593124\n",
      "205 0.01948270946741104\n",
      "206 0.02259203791618347\n",
      "207 0.022896042093634605\n",
      "208 0.02053876779973507\n",
      "209 0.024777287617325783\n",
      "210 0.022345975041389465\n",
      "211 0.02348732389509678\n",
      "212 0.023572921752929688\n",
      "213 0.02140016295015812\n",
      "214 0.0216029342263937\n",
      "215 0.021799717098474503\n",
      "216 0.0222189761698246\n",
      "217 0.023058831691741943\n",
      "218 0.02351098321378231\n",
      "219 0.02694842219352722\n",
      "220 0.023646362125873566\n",
      "221 0.021190496161580086\n",
      "222 0.023892518132925034\n",
      "223 0.02318437770009041\n",
      "224 0.023440465331077576\n",
      "225 0.02260233275592327\n",
      "226 0.023074565455317497\n",
      "227 0.021881261840462685\n",
      "228 0.020903147757053375\n",
      "229 0.022981960326433182\n",
      "230 0.021599896252155304\n",
      "231 0.02280053310096264\n",
      "232 0.02136559970676899\n",
      "233 0.02428288571536541\n",
      "234 0.021669626235961914\n",
      "235 0.023226892575621605\n",
      "236 0.02247813157737255\n",
      "237 0.025318874046206474\n",
      "238 0.02151990309357643\n",
      "239 0.02346942387521267\n",
      "240 0.021466882899403572\n",
      "241 0.022798046469688416\n",
      "242 0.02076943777501583\n",
      "243 0.022339623421430588\n",
      "244 0.022494304925203323\n",
      "245 0.021655546501278877\n",
      "246 0.02339443750679493\n",
      "247 0.024920625612139702\n",
      "248 0.023203391581773758\n",
      "249 0.022109510377049446\n",
      "250 0.023404762148857117\n",
      "251 0.02531999908387661\n",
      "252 0.022486573085188866\n",
      "253 0.02242961712181568\n",
      "254 0.02252971939742565\n",
      "255 0.024512017145752907\n",
      "256 0.02298176847398281\n",
      "257 0.02446800284087658\n",
      "258 0.02013951912522316\n",
      "259 0.02341851219534874\n",
      "260 0.025225266814231873\n",
      "261 0.02183610387146473\n",
      "262 0.022781046107411385\n",
      "263 0.022618012502789497\n",
      "264 0.022885510697960854\n",
      "265 0.022568929940462112\n",
      "266 0.021571841090917587\n",
      "267 0.020623676478862762\n",
      "268 0.0229469183832407\n",
      "269 0.022135460749268532\n",
      "270 0.02318725921213627\n",
      "271 0.021626723930239677\n",
      "272 0.023473137989640236\n",
      "273 0.02398860827088356\n",
      "274 0.021607939153909683\n",
      "275 0.02363157644867897\n",
      "276 0.022128595039248466\n",
      "277 0.021608157083392143\n",
      "278 0.023079127073287964\n",
      "279 0.0229365024715662\n",
      "280 0.021527959033846855\n",
      "281 0.022145645692944527\n",
      "282 0.020773444324731827\n",
      "283 0.021029682829976082\n",
      "284 0.023131664842367172\n",
      "285 0.022884275764226913\n",
      "286 0.02113908715546131\n",
      "287 0.023382561281323433\n",
      "288 0.023184053599834442\n",
      "289 0.020810892805457115\n",
      "290 0.02315465919673443\n",
      "291 0.02306016907095909\n",
      "292 0.023480240255594254\n",
      "293 0.021952999755740166\n",
      "294 0.021797960624098778\n",
      "295 0.021738383919000626\n",
      "296 0.022575631737709045\n",
      "297 0.021525636315345764\n",
      "298 0.024463092908263206\n",
      "299 0.024932101368904114\n",
      "300 0.020832467824220657\n",
      "301 0.02211785316467285\n",
      "302 0.023391101509332657\n",
      "303 0.018865492194890976\n",
      "304 0.023558329790830612\n",
      "305 0.022535143420100212\n",
      "306 0.02272644452750683\n",
      "307 0.02339126542210579\n",
      "308 0.023995447903871536\n",
      "309 0.02381850592792034\n",
      "310 0.0215445626527071\n",
      "311 0.0228823721408844\n",
      "312 0.024285703897476196\n",
      "313 0.020191840827465057\n",
      "314 0.02318481355905533\n",
      "315 0.020790044218301773\n",
      "316 0.022497044876217842\n",
      "317 0.022789549082517624\n",
      "318 0.019865360110998154\n",
      "319 0.022428080439567566\n",
      "320 0.021430961787700653\n",
      "321 0.02440202422440052\n",
      "322 0.02464594691991806\n",
      "323 0.020960312336683273\n",
      "324 0.022989999502897263\n",
      "325 0.024054551497101784\n",
      "326 0.021889735013246536\n",
      "327 0.023518377915024757\n",
      "328 0.019127728417515755\n",
      "329 0.023492448031902313\n",
      "330 0.023174606263637543\n",
      "331 0.022204367443919182\n",
      "332 0.02390163764357567\n",
      "333 0.02428796887397766\n",
      "334 0.021566301584243774\n",
      "335 0.023171741515398026\n",
      "336 0.022083884105086327\n",
      "337 0.022190067917108536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338 0.024554356932640076\n",
      "339 0.021125182509422302\n",
      "340 0.02081279829144478\n",
      "341 0.021791130304336548\n",
      "342 0.02172834612429142\n",
      "343 0.021340904757380486\n",
      "344 0.024392636492848396\n",
      "345 0.022142978385090828\n",
      "346 0.02368086948990822\n",
      "347 0.02344701811671257\n",
      "348 0.02167026326060295\n",
      "349 0.022661268711090088\n",
      "350 0.02358611300587654\n",
      "351 0.021893152967095375\n",
      "352 0.021914055570960045\n",
      "353 0.022352205589413643\n",
      "354 0.021395012736320496\n",
      "355 0.02091001532971859\n",
      "356 0.019794469699263573\n",
      "357 0.020154280588030815\n",
      "358 0.022168368101119995\n",
      "359 0.02251810021698475\n",
      "360 0.02427622117102146\n",
      "361 0.021778812631964684\n",
      "362 0.023538541048765182\n",
      "363 0.019926361739635468\n",
      "364 0.020464381203055382\n",
      "365 0.0211489275097847\n",
      "366 0.021440884098410606\n",
      "367 0.022309815511107445\n",
      "368 0.022372350096702576\n",
      "369 0.021567273885011673\n",
      "370 0.022497011348605156\n",
      "371 0.023387735709547997\n",
      "372 0.023825019598007202\n",
      "373 0.024781253188848495\n",
      "374 0.023304995149374008\n",
      "375 0.021476926282048225\n",
      "376 0.02065306529402733\n",
      "377 0.022474532946944237\n",
      "378 0.02226541005074978\n",
      "379 0.023450691252946854\n",
      "380 0.02293838933110237\n",
      "381 0.023092804476618767\n",
      "done\n",
      "7.973254387204765e-05\n",
      "0.02250560767051437\n",
      "0.009638505724517151\n",
      "1.0451202601662481\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "ce_losses = []\n",
    "recon_losses = []\n",
    "kl_losses = []\n",
    "sample_stds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = to_cuda(batch)\n",
    "        stt_outputs, tts_outputs = model(batch)\n",
    "        ce_loss = stt_outputs['loss'].item()\n",
    "        recon_loss = tts_outputs['recon_loss'].item()\n",
    "        kl_loss = tts_outputs['kl_loss'].item()\n",
    "        print(i, recon_loss)\n",
    "        \n",
    "        ce_losses.append(ce_loss)\n",
    "        recon_losses.append(recon_loss)\n",
    "        kl_losses.append(kl_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        samples_list = []\n",
    "        for _ in range(10):\n",
    "            samples, _ = model.inference(batch['text'], batch['mels'].size(2), stt_outputs[\"alignments\"], temperature=1.0, clip=2)\n",
    "            samples_list.append(samples)\n",
    "        samples_list = torch.cat(samples_list, dim=0)\n",
    "        sample_std = torch.std(samples_list, dim=0).mean().item()\n",
    "        sample_stds.append(sample_std)\n",
    "            \n",
    "        \n",
    "print('done')\n",
    "\n",
    "print(np.mean(ce_losses))\n",
    "print(np.mean(recon_losses))\n",
    "print(np.mean(kl_losses))\n",
    "print(np.mean(sample_stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
