{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from hparams.hparams_S4L import create_hparams\n",
    "from model import Model\n",
    "from datasets import LJDataset, TextMelCollate\n",
    "from utils import sizeof_fmt, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 13 21:58:21 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:2E:00.0 Off |                  N/A |\n",
      "| 57%   65C    P2    96W / 250W |    256MiB / 11016MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:2F:00.0 Off |                  N/A |\n",
      "| 37%   59C    P0    50W / 250W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     21362      C   ...onda3/envs/vae/bin/python      253MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "save_dir = '/data/save/model_S4L'\n",
    "logger = Logger(save_dir=save_dir, new=False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.json    save_100817  save_65000  save_75000  save_85000  save_94076\r\n",
      "save_100000  save_60377   save_70000  save_80000  save_90000  save_95000\r\n"
     ]
    }
   ],
   "source": [
    "!ls $save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size 243.8MiB\n",
      "TTS size 156.2MiB\n",
      "MelDecoder size 94.9MiB\n",
      "loaded : 100000\n",
      "100000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "stt_hparams, tts_hparams = create_hparams()\n",
    "model = Model(stt_hparams, tts_hparams, mode='train')\n",
    "model = model.cuda()\n",
    "step = 100000\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.parameters()))\n",
    "print(f\"Model size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.parameters()))\n",
    "print(f\"TTS size {size}\")\n",
    "\n",
    "size = sizeof_fmt(4 * sum(p.numel() for p in model.tts.mel_decoder.parameters()))\n",
    "print(f\"MelDecoder size {size}\")\n",
    "\n",
    "if True:\n",
    "    model, _, _ = logger.load(step, model, None)\n",
    "print(step)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7ff786e29ac0>\n"
     ]
    }
   ],
   "source": [
    "testset = LJDataset(tts_hparams, split='test')\n",
    "collate_fn = TextMelCollate(tts_hparams)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, num_workers=1, \n",
    "                          shuffle=False, sampler=None, batch_size=1, pin_memory=False,\n",
    "                          drop_last=True, collate_fn=collate_fn)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cuda(batch):\n",
    "    batch['text'] = batch['text'].cuda()\n",
    "    batch['text_lengths'] = batch['text_lengths'].cuda()\n",
    "    batch['mels'] = batch['mels'].cuda()\n",
    "    batch['mel_lengths'] = batch['mel_lengths'].cuda()\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.053579915314912796\n",
      "1 0.05577331408858299\n",
      "2 0.05418180674314499\n",
      "3 0.0505235530436039\n",
      "4 0.05432279035449028\n",
      "5 0.05032835900783539\n",
      "6 0.053301308304071426\n",
      "7 0.052974503487348557\n",
      "8 0.05195525661110878\n",
      "9 0.05141054466366768\n",
      "10 0.051849573850631714\n",
      "11 0.04942905530333519\n",
      "12 0.049922797828912735\n",
      "13 0.053881313651800156\n",
      "14 0.05044994503259659\n",
      "15 0.052898701280355453\n",
      "16 0.06357131153345108\n",
      "17 0.05134565010666847\n",
      "18 0.05227970331907272\n",
      "19 0.0526837520301342\n",
      "20 0.052169203758239746\n",
      "21 0.05378590524196625\n",
      "22 0.048659149557352066\n",
      "23 0.05182451009750366\n",
      "24 0.054230693727731705\n",
      "25 0.052053097635507584\n",
      "26 0.051507368683815\n",
      "27 0.04693121835589409\n",
      "28 0.05309556424617767\n",
      "29 0.0522792674601078\n",
      "30 0.05274364352226257\n",
      "31 0.05327274277806282\n",
      "32 0.051350805908441544\n",
      "33 0.0489443801343441\n",
      "34 0.06983238458633423\n",
      "35 0.05263146385550499\n",
      "36 0.05331539362668991\n",
      "37 0.05516939237713814\n",
      "38 0.052530910819768906\n",
      "39 0.049828317016363144\n",
      "40 0.05102500319480896\n",
      "41 0.05195726826786995\n",
      "42 0.05799698457121849\n",
      "43 0.058268819004297256\n",
      "44 0.054985325783491135\n",
      "45 0.05033394321799278\n",
      "46 0.05013502016663551\n",
      "47 0.04960025101900101\n",
      "48 0.05203159525990486\n",
      "49 0.045299794524908066\n",
      "50 0.05143977329134941\n",
      "51 0.05075354874134064\n",
      "52 0.05463626980781555\n",
      "53 0.051904864609241486\n",
      "54 0.056940071284770966\n",
      "55 0.05147620663046837\n",
      "56 0.05085023120045662\n",
      "57 0.058867864310741425\n",
      "58 0.04951287433505058\n",
      "59 0.056083861738443375\n",
      "60 0.049154773354530334\n",
      "61 0.050321437418460846\n",
      "62 0.0508904755115509\n",
      "63 0.05179053917527199\n",
      "64 0.04733111709356308\n",
      "65 0.053747959434986115\n",
      "66 0.04380542412400246\n",
      "67 0.05020813271403313\n",
      "68 0.05514480173587799\n",
      "69 0.05090152099728584\n",
      "70 0.05332091823220253\n",
      "71 0.05195140466094017\n",
      "72 0.04829294607043266\n",
      "73 0.05256091058254242\n",
      "74 0.04750550165772438\n",
      "75 0.05202995613217354\n",
      "76 0.053069815039634705\n",
      "77 0.05236899107694626\n",
      "78 0.04976241663098335\n",
      "79 0.04778638482093811\n",
      "80 0.05245519056916237\n",
      "81 0.05092623457312584\n",
      "82 0.05016865208745003\n",
      "83 0.04880843684077263\n",
      "84 0.048740021884441376\n",
      "85 0.04934745281934738\n",
      "86 0.04969638213515282\n",
      "87 0.04894464835524559\n",
      "88 0.047914084047079086\n",
      "89 0.05022420361638069\n",
      "90 0.05170878395438194\n",
      "91 0.05469966307282448\n",
      "92 0.051171381026506424\n",
      "93 0.05356140807271004\n",
      "94 0.047844767570495605\n",
      "95 0.05117735639214516\n",
      "96 0.05081991106271744\n",
      "97 0.050051625818014145\n",
      "98 0.04685094207525253\n",
      "99 0.048196885734796524\n",
      "100 0.04894571378827095\n",
      "101 0.04773838818073273\n",
      "102 0.04715559631586075\n",
      "103 0.0532074011862278\n",
      "104 0.048280972987413406\n",
      "105 0.0489455908536911\n",
      "106 0.0655486062169075\n",
      "107 0.04970063269138336\n",
      "108 0.055828072130680084\n",
      "109 0.04916716739535332\n",
      "110 0.04661329463124275\n",
      "111 0.05037274211645126\n",
      "112 0.048992130905389786\n",
      "113 0.0487414114177227\n",
      "114 0.048216160386800766\n",
      "115 0.046913329511880875\n",
      "116 0.046872690320014954\n",
      "117 0.04948343709111214\n",
      "118 0.04709163308143616\n",
      "119 0.049654554575681686\n",
      "120 0.047213226556777954\n",
      "121 0.04804369807243347\n",
      "122 0.05030267313122749\n",
      "123 0.05047788843512535\n",
      "124 0.047101084142923355\n",
      "125 0.048615120351314545\n",
      "126 0.04798152670264244\n",
      "127 0.044901687651872635\n",
      "128 0.04475903511047363\n",
      "129 0.04387570917606354\n",
      "130 0.04784834012389183\n",
      "131 0.04855598136782646\n",
      "132 0.04938613623380661\n",
      "133 0.05506688356399536\n",
      "134 0.050663355737924576\n",
      "135 0.044389769434928894\n",
      "136 0.04863804206252098\n",
      "137 0.04970546439290047\n",
      "138 0.04619114100933075\n",
      "139 0.044617339968681335\n",
      "140 0.051934290677309036\n",
      "141 0.05388323590159416\n",
      "142 0.05317661538720131\n",
      "143 0.050729282200336456\n",
      "144 0.05206995829939842\n",
      "145 0.047726817429065704\n",
      "146 0.050796981900930405\n",
      "147 0.04731554538011551\n",
      "148 0.04929173365235329\n",
      "149 0.047373998910188675\n",
      "150 0.05066123604774475\n",
      "151 0.05028563365340233\n",
      "152 0.0482119657099247\n",
      "153 0.05119346082210541\n",
      "154 0.04992556944489479\n",
      "155 0.04717164859175682\n",
      "156 0.04684258624911308\n",
      "157 0.049273308366537094\n",
      "158 0.04736819118261337\n",
      "159 0.04917142540216446\n",
      "160 0.04578132927417755\n",
      "161 0.046428196132183075\n",
      "162 0.048338692635297775\n",
      "163 0.049720533192157745\n",
      "164 0.047860078513622284\n",
      "165 0.05131087824702263\n",
      "166 0.047363508492708206\n",
      "167 0.05090329051017761\n",
      "168 0.04817473888397217\n",
      "169 0.049822207540273666\n",
      "170 0.04902879521250725\n",
      "171 0.049711693078279495\n",
      "172 0.04851705953478813\n",
      "173 0.04323052987456322\n",
      "174 0.05061366781592369\n",
      "175 0.046380769461393356\n",
      "176 0.0469675175845623\n",
      "177 0.0472729429602623\n",
      "178 0.04756157100200653\n",
      "179 0.050075892359018326\n",
      "180 0.049648772925138474\n",
      "181 0.04983782023191452\n",
      "182 0.049106307327747345\n",
      "183 0.047426048666238785\n",
      "184 0.045786064118146896\n",
      "185 0.045657988637685776\n",
      "186 0.04883283004164696\n",
      "187 0.0487709604203701\n",
      "188 0.044746872037649155\n",
      "189 0.04881377890706062\n",
      "190 0.04919964075088501\n",
      "191 0.04809318482875824\n",
      "192 0.04947103187441826\n",
      "193 0.04582082852721214\n",
      "194 0.04619551822543144\n",
      "195 0.045163121074438095\n",
      "196 0.04987047612667084\n",
      "197 0.04706496000289917\n",
      "198 0.04624685272574425\n",
      "199 0.045872777700424194\n",
      "200 0.042989734560251236\n",
      "201 0.04348328709602356\n",
      "202 0.04602733626961708\n",
      "203 0.04457738250494003\n",
      "204 0.051226161420345306\n",
      "205 0.048802509903907776\n",
      "206 0.05252663418650627\n",
      "207 0.04970090836286545\n",
      "208 0.04648486524820328\n",
      "209 0.04470312222838402\n",
      "210 0.04906626418232918\n",
      "211 0.04830559343099594\n",
      "212 0.0489073172211647\n",
      "213 0.04383864998817444\n",
      "214 0.044964347034692764\n",
      "215 0.04524329677224159\n",
      "216 0.0456317737698555\n",
      "217 0.04783877730369568\n",
      "218 0.04770943894982338\n",
      "219 0.04454309120774269\n",
      "220 0.050137002021074295\n",
      "221 0.045005183666944504\n",
      "222 0.04468951374292374\n",
      "223 0.044439781457185745\n",
      "224 0.051282916218042374\n",
      "225 0.05027012526988983\n",
      "226 0.05038504675030708\n",
      "227 0.04653682932257652\n",
      "228 0.046573247760534286\n",
      "229 0.04905320331454277\n",
      "230 0.04956714063882828\n",
      "231 0.04722687602043152\n",
      "232 0.04609357938170433\n",
      "233 0.04878639057278633\n",
      "234 0.045062970370054245\n",
      "235 0.04903566464781761\n",
      "236 0.047333844006061554\n",
      "237 0.04487025365233421\n",
      "238 0.04863728582859039\n",
      "239 0.04756515845656395\n",
      "240 0.049079909920692444\n",
      "241 0.04920932650566101\n",
      "242 0.04790513217449188\n",
      "243 0.05077064782381058\n",
      "244 0.04854587838053703\n",
      "245 0.04813743755221367\n",
      "246 0.050081945955753326\n",
      "247 0.04554019123315811\n",
      "248 0.042370546609163284\n",
      "249 0.04750419408082962\n",
      "250 0.044318050146102905\n",
      "251 0.04388831555843353\n",
      "252 0.05083196237683296\n",
      "253 0.04394448921084404\n",
      "254 0.0475415475666523\n",
      "255 0.04646698758006096\n",
      "256 0.046963904052972794\n",
      "257 0.047809913754463196\n",
      "258 0.049273550510406494\n",
      "259 0.0454733707010746\n",
      "260 0.049038127064704895\n",
      "261 0.046118542551994324\n",
      "262 0.04959776997566223\n",
      "263 0.05011169984936714\n",
      "264 0.04986577108502388\n",
      "265 0.04568430781364441\n",
      "266 0.05196237564086914\n",
      "267 0.04770367592573166\n",
      "268 0.04626297205686569\n",
      "269 0.0457460954785347\n",
      "270 0.05193079635500908\n",
      "271 0.048109255731105804\n",
      "272 0.05042169988155365\n",
      "273 0.04574000462889671\n",
      "274 0.0457838773727417\n",
      "275 0.04871300980448723\n",
      "276 0.04918389022350311\n",
      "277 0.04826391860842705\n",
      "278 0.048806603997945786\n",
      "279 0.04420577734708786\n",
      "280 0.04224323108792305\n",
      "281 0.04462376609444618\n",
      "282 0.04817694053053856\n",
      "283 0.04996003955602646\n",
      "284 0.046242792159318924\n",
      "285 0.046425070613622665\n",
      "286 0.04935076832771301\n",
      "287 0.046873558312654495\n",
      "288 0.049278050661087036\n",
      "289 0.04566894471645355\n",
      "290 0.04740148037672043\n",
      "291 0.048826251178979874\n",
      "292 0.047689199447631836\n",
      "293 0.048289041966199875\n",
      "294 0.05271788686513901\n",
      "295 0.041916243731975555\n",
      "296 0.048593346029520035\n",
      "297 0.04481580853462219\n",
      "298 0.04532758519053459\n",
      "299 0.04797957465052605\n",
      "300 0.044292598962783813\n",
      "301 0.04607631638646126\n",
      "302 0.04820175841450691\n",
      "303 0.0512075312435627\n",
      "304 0.045234911143779755\n",
      "305 0.04235779494047165\n",
      "306 0.04689937084913254\n",
      "307 0.044357508420944214\n",
      "308 0.047106388956308365\n",
      "309 0.051437683403491974\n",
      "310 0.045361459255218506\n",
      "311 0.04664260149002075\n",
      "312 0.046136289834976196\n",
      "313 0.04516352340579033\n",
      "314 0.04806962236762047\n",
      "315 0.0456039234995842\n",
      "316 0.04312673956155777\n",
      "317 0.04571832716464996\n",
      "318 0.044922471046447754\n",
      "319 0.0463506355881691\n",
      "320 0.045078739523887634\n",
      "321 0.0458553321659565\n",
      "322 0.05213560163974762\n",
      "323 0.04474601522088051\n",
      "324 0.046280402690172195\n",
      "325 0.04858921095728874\n",
      "326 0.0467977300286293\n",
      "327 0.046014249324798584\n",
      "328 0.04644809663295746\n",
      "329 0.04856672137975693\n",
      "330 0.04761696234345436\n",
      "331 0.04860646650195122\n",
      "332 0.049906279891729355\n",
      "333 0.04771699756383896\n",
      "334 0.04413903132081032\n",
      "335 0.045689646154642105\n",
      "336 0.04519840329885483\n",
      "337 0.05263015627861023\n",
      "338 0.04643658548593521\n",
      "339 0.044545043259859085\n",
      "340 0.047576066106557846\n",
      "341 0.0466182641685009\n",
      "342 0.04721952602267265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343 0.04273485019803047\n",
      "344 0.04578794538974762\n",
      "345 0.04235602915287018\n",
      "346 0.04523347318172455\n",
      "347 0.04936463385820389\n",
      "done\n",
      "0.13321736186010943\n",
      "0.04891854572784284\n",
      "0.015724523621878917\n",
      "0.27639047394710026\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "ce_losses = []\n",
    "recon_losses = []\n",
    "kl_losses = []\n",
    "sample_stds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        batch = to_cuda(batch)\n",
    "        stt_outputs, tts_outputs = model(batch)\n",
    "        ce_loss = stt_outputs['loss'].item()\n",
    "        recon_loss = tts_outputs['recon_loss'].item()\n",
    "        kl_loss = tts_outputs['kl_loss'].item()\n",
    "        print(i, recon_loss)\n",
    "        \n",
    "        ce_losses.append(ce_loss)\n",
    "        recon_losses.append(recon_loss)\n",
    "        kl_losses.append(kl_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        samples_list = []\n",
    "        for _ in range(10):\n",
    "            samples, _ = model.inference(batch['text'], batch['mels'].size(2), stt_outputs[\"alignments\"], temperature=1.0, clip=2)\n",
    "            samples_list.append(samples)\n",
    "        samples_list = torch.cat(samples_list, dim=0)\n",
    "        sample_std = torch.std(samples_list, dim=0).mean().item()\n",
    "        sample_stds.append(sample_std)\n",
    "            \n",
    "        \n",
    "print('done')\n",
    "\n",
    "print(np.mean(ce_losses))\n",
    "print(np.mean(recon_losses))\n",
    "print(np.mean(kl_losses))\n",
    "print(np.mean(sample_stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
